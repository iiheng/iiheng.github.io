### 思路：
1. 输入关键词跳转到搜索结果页
2. 解析内容并保存
3. 点击下一页
5. 循环直至无法点击

### 当当网抓取
#### 库导入
```commandline
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from pyquery import PyQuery as pq
from selenium.webdriver.common.keys import Keys
import logging
```

#### 配置
```commandline
logging.basicConfig(level=logging.INFO,format="%(asctime)s - %(levelname)s : %(message)s")

options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_argument('--disable-gpu')
options.add_argument("--no-sandbox")
options.add_argument('-ignore-certificate-errors')
options.add_argument('-ignore -ssl-errors')
browser = webdriver.Chrome(options=options)

NEXT = (By.CSS_SELECTOR,".paging .next a") # 下一页的按钮
wait = WebDriverWait(browser,50)
df = pd.DataFrame(columns=["name",'price',"introduce"])
```

#### 输入搜索并跳转
```commandline
# 输入关键词搜索
def search(key):
    """
    :param key:
    :return: html
    """
    input = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,"#key_S")))
    input.send_keys(key)
    time.sleep(0.5)
    input.send_keys(Keys.ENTER)
    browser.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    logging.info("searching %s",key)
```
#### 加载全部内容以及点击下一页
```commandline
# 加载全部内容，如果页面由ajax加载
def loadAll():
    buttonl = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,'.next')))
    browser.execute_script('arguments[0].scrollIntoView(true);', buttonl)
    logging.info("All loading")

# 点击下一页按钮
def clickNext():
    """
    :return:
    """
    try:
        time.sleep(3)
        buttonc = wait.until(EC.presence_of_element_located(NEXT))
        browser.execute_script("arguments[0].click();",buttonc)
        logging.info("next page")
        return True
    except Exception as e:
        logging.info(e)
        browser.close()
        return False
```
#### 解析页面并保存
```commandline
# 解析页面返回信息
def parse_html(html):
    """
    :param html:
    :return: info
    """
    doc = pq(html)
    items = doc('div #search_nature_rg ul li').items()
    page = doc('.current').text()
    logging.info("current page %s",page)
    for item in items:
        name = item('.name').text()
        price = item('.search_now_price').text()
        intr = item('.detail').text()
        yield {
            "name":name,
            "price":price,
            "intr":intr
        }

# 保存数据
def save_data(items):
    try:
        for item in items:
            df.loc[len(df)] = {
                'name': item["name"],
                'price': item['price'],
                'introduce': item['intr'],
            }
        df.to_csv('ces.csv',encoding='utf-8',mode="a")
        logging.info("save data successfully")
    except Exception as e:
        logging.info(e)
```

#### 最后运行
```commandline
def main():
    browser.get("http://www.dangdang.com/")
    key = "python"
    search(key)
    while True:
        loadAll()
        html = browser.page_source
        items = parse_html(html)
        save_data(items)
        if not clickNext():break

if __name__ == "__main__":
    main()
```

### 猎聘网抓取同理
#### 修改网址
在mian()方法中修改网址，并加上跳转到新窗口的代码
```commandline
browser.get("http://www.liepin.com/")
handles = browser.window_handles
browser.switch_to.window(handles[1])
```
#### 修改NEXT
```commandline
NEXT = (By.CSS_SELECTOR,'span[aria-label="right"]') # 下一页的按钮
```

#### 最后修改parse_html()即可，运行参考当当网
