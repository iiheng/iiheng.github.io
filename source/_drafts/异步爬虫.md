---
title: 异步爬虫
date: 2022-02-22 17:54:08
tags: 异步爬虫
categories: python爬虫的学习
---
## 前言
- 爬虫文章均来自[《python3网络爬虫开发实战教程》](https://item.jd.com/12333540.html)，只为方便记录日常所学，以及方便观看。

## 协程的基本原理
### 案例引入
```
import requests
import logging
import time

logging.basicConfig(level=logging.INFO,format="%(asctime)s - %(levelname)s:%(message)s")
TOTAL_NUMBER = 100
url = "https://www.httpbin.org/delay/5"

start_time = time.time()
for _ in range(1,TOTAL_NUMBER+1):
    logging.info('scraping %s',url)
    response = requests.get(url)
    logging.info("这是第 %s 次",_)
end_time = time.time()
logging.info("total time %s seconds",end_time-start_time)
```
### 基础知识
#### 阻塞
#### 非阻塞
#### 同步
#### 异步
#### 多进程
多进程就是利用CPU的多核优势，在同一时间并行执行多个任务，可以大大提高执行效率
#### 协程
### 协程的用法
#### 准备工作
#### 定义协程
```
import asyncio

async def execute(x):
    print('Number:',x)

coroutin = execute(1)
print('Coroutine',coroutin)
print('After calling execute')

loop = asyncio.get_event_loop()
loop.run_until_complete(coroutin)
print('After calling loop')
```

```
import asyncio

async def execute(x):
    print('Number:',x)
    return x

coroutin = execute(1)
print('Coroutine',coroutin)
print('After calling execute')

loop = asyncio.get_event_loop()
task = loop.create_task(coroutin)
print('Task:',task)
loop.run_until_complete(task)
print("Task:",task)
print('After calling loop')
```

	Coroutine <coroutine object execute at 0x000002A7F23A8C40>
	After calling execute
	Task: <Task pending name='Task-1' coro=<execute() running at E:\pythonproject\scrapel\parsehtmll.py:3>>
	Number: 1
	Task: <Task finished name='Task-1' coro=<execute() done, defined at E:\pythonproject\scrapel\parsehtmll.py:3> result=1>
	After calling loop
	
```
import asyncio

async def execute(x):
    print('Number:',x)
    return x

coroutin = execute(1)
print('Coroutine',coroutin)
print('After calling execute')

task = asyncio.ensure_future(coroutin)
loop = asyncio.get_event_loop()
print('Task:',task)
loop.run_until_complete(task)
print("Task:",task)
print('After calling loop')
```

	Coroutine <coroutine object execute at 0x00000245836C8C40>
	After calling execute
	Task: <Task pending name='Task-1' coro=<execute() running at E:\pythonproject\scrapel\parsehtmll.py:3>>
	Number: 1
	Task: <Task finished name='Task-1' coro=<execute() done, defined at E:\pythonproject\scrapel\parsehtmll.py:3> result=1>
	After calling loop

### 绑定回调
```
import asyncio
import requests

async def request():
    url = 'https://www.baidu.com'
    status = requests.get(url)
    return status

def callback(task):
    print('Status:',task.result)

coroutin = request()
task = asyncio.ensure_future(coroutin)
task.add_done_callback(callback)
print('Task:',task)

loop = asyncio.get_event_loop()
loop.run_until_complete(task)
print('Task:',task)
```

	Task: <Task pending name='Task-1' coro=<request() running at E:\pythonproject\scrapel\parsehtmll.py:4> cb=[callback() at E:\pythonproject\scrapel\parsehtmll.py:9]>
	Status: <built-in method result of _asyncio.Task object at 0x000001A700348EE0>
	Task: <Task finished name='Task-1' coro=<request() done, defined at E:\pythonproject\scrapel\parsehtmll.py:4> result=<Response [200]>>
	
```
import asyncio
import requests

async def request():
    url = 'https://www.baidu.com'
    status = requests.get(url)
    return status


coroutin = request()
task = asyncio.ensure_future(coroutin)
print('Task:',task)

loop = asyncio.get_event_loop()
loop.run_until_complete(task)
print('Task:',task)
print('Task Result:',task.result())
```

	Task: <Task pending name='Task-1' coro=<request() running at E:\pythonproject\scrapel\parsehtmll.py:4>>
	Task: <Task finished name='Task-1' coro=<request() done, defined at E:\pythonproject\scrapel\parsehtmll.py:4> result=<Response [200]>>
	Task Result: <Response [200]>
	
### 多任务协程
```
import asyncio
import requests

async def request():
    url = 'https://www.baidu.com'
    status = requests.get(url)
    return status


tasks = [asyncio.ensure_future(request()) for _ in range(5)]
print('Tasks:',tasks)

loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))

for task in tasks:
    print('Task Result:',task.result())
```

	Tasks: [<Task pending name='Task-1' coro=<request() running at E:\pythonproject\scrapel\parsehtmll.py:4>>, <Task pending name='Task-2' coro=<request() running at E:\pythonproject\scrapel\parsehtmll.py:4>>, <Task pending name='Task-3' coro=<request() running at E:\pythonproject\scrapel\parsehtmll.py:4>>, <Task pending name='Task-4' coro=<request() running at E:\pythonproject\scrapel\parsehtmll.py:4>>, <Task pending name='Task-5' coro=<request() running at E:\pythonproject\scrapel\parsehtmll.py:4>>]
	Task Result: <Response [200]>
	Task Result: <Response [200]>
	Task Result: <Response [200]>
	Task Result: <Response [200]>
	Task Result: <Response [200]>
	
```
import asyncio
import requests
import time

start_time = time.time()

async def get(url):
    return requests.get(url)

async def request():
    url = 'https://www.httpbin.org/delay/5'
    print('Waiting for',url)
    response = await get(url)
    print('Get response from',url,'response',response)

task = [asyncio.ensure_future(request()) for _ in range(10)]
loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(task))

end_time = time.time()
print('Cost time:',end_time - start_time)
```

	Waiting for https://www.httpbin.org/delay/5
	Get response from https://www.httpbin.org/delay/5 response <Response [200]>
	Waiting for https://www.httpbin.org/delay/5
	Get response from https://www.httpbin.org/delay/5 response <Response [200]>
	Waiting for https://www.httpbin.org/delay/5
	Get response from https://www.httpbin.org/delay/5 response <Response [200]>
	Cost time: 62.92758226394653
	
### 使用aiohttp
```
import asyncio
import aiohttp
import time

start_time = time.time()

async def get(url):
    session = aiohttp.ClientSession()
    response = await session.get(url)
    await response.text()
    await session.close()
    return response

async def request():
    url = 'https://www.httpbin.org/delay/5'
    print('Waiting for',url)
    response = await get(url)
    print('Get response from',url,'response',response)

task = [asyncio.ensure_future(request()) for _ in range(10)]
loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(task))

end_time = time.time()
print('Cost time:',end_time - start_time)
```
```
import asyncio
import aiohttp
import time

def test(number):
    start = time.time()

    async def get(url):
        session = aiohttp.ClientSession()
        response = await session.get(url)
        await response.text()
        await session.close()
        return response

    async def request():
        url = 'https://www.baidu.com/'
        await get(url)

    tasks = [asyncio.ensure_future(request()) for _ in range(number)]
    loop = asyncio.get_event_loop()
    loop.run_until_complete(asyncio.wait(tasks))
    end = time.time()
    print('Number:',number,'Cost time:',end - start)

for number in [1,3,5,10,15,30,50,75,100,200,500]:
    test(number)
```
## aiohttp的使用
### 基本介绍
前面介绍的asyncio模块，其内部实现了对TCP,UDP,SSL协议的异步操作，但是对于HTTP请求来说，就需要用aiohttp实现了。
### 基本实例
```
import aiohttp
import asyncio

async def fetch(session,url):
    async with session.get(url) as response:
        return await response.text(),response.status

async def main():
    async with aiohttp.ClientSession() as session:
        html,status = await fetch(session,'https://cuiqingcai.com')
        print(f"html:{html[:100]}...")
        print(f'status:{status}')

if __name__ == "__main__":
    loop = asyncio.get_event_loop()
    loop.run_until_complete(main())
```

	html:<!DOCTYPE html>
	<html lang="zh-CN">

	<head>
	  <meta charset="UTF-8">
	  <meta name="viewport" content...
	status:200

### URL参数设置
`session.post('http://www.httpbin.org/post')`
`session.put('http://www.httpbin.org/put',data=b'data')`
`session.delete('http://www.httpbin.org/delete')`
`session.head('http://www.httpbin.org/get')`
`session.options('http://www.httpbin.org/get')`
`session.patch('http://www.httpbin.org/patch',data = b'data')`
### POST请求
```
import aiohttp
import asyncio

async def main():
    date = {'name':'iheng','age':23}
    async with aiohttp.ClientSession() as session:
        async with session.post('https://www.httpbin.org/post',data=date) as response:
            print(await response.text())

if __name__=="__main__":
    asyncio.get_event_loop().run_until_complete(main())
```
	{
	  "args": {}, 
	  "data": "", 
	  "files": {}, 
	  "form": {
		"age": "23", 
		"name": "iheng"
	  }, 
	  "headers": {
		"Accept": "*/*", 
		"Accept-Encoding": "gzip, deflate", 
		"Content-Length": "17", 
		"Content-Type": "application/x-www-form-urlencoded", 
		"Host": "www.httpbin.org", 
		"User-Agent": "Python/3.9 aiohttp/3.8.1", 
		"X-Amzn-Trace-Id": "Root=1-6214f80f-50abb2e6207b6b5f318de57d"
	  }, 
	  "json": null, 
	  "origin": "36.28.12.6", 
	  "url": "https://www.httpbin.org/post"
	}
	
```
async def main():
    date = {'name':'iheng','age':23}
    async with aiohttp.ClientSession() as session:
        async with session.post('https://www.httpbin.org/post',json=date) as response:
            print(await response.text())
```
	{
	  "args": {}, 
	  "data": "{\"name\": \"iheng\", \"age\": 23}", 
	  "files": {}, 
	  "form": {}, 
	  "headers": {
		"Accept": "*/*", 
		"Accept-Encoding": "gzip, deflate", 
		"Content-Length": "28", 
		"Content-Type": "application/json", 
		"Host": "www.httpbin.org", 
		"User-Agent": "Python/3.9 aiohttp/3.8.1", 
		"X-Amzn-Trace-Id": "Root=1-6214f891-011f326c02a0f7097929dea4"
	  }, 
	  "json": {
		"age": 23, 
		"name": "iheng"
	  }, 
	  "origin": "36.28.12.6", 
	  "url": "https://www.httpbin.org/post"
	}
### 响应
```
import aiohttp
import asyncio

async def main():
    date = {'name':'iheng','age':23}
    async with aiohttp.ClientSession() as session:
        async with session.post('https://www.httpbin.org/post',json=date) as response:
            print('status:',response.status)
            print('headers:',response.headers)
            print('body:',await response.text())
            print('bytes:',await response.read())
            print('bytes:',await response.json())

if __name__=="__main__":
    asyncio.get_event_loop().run_until_complete(main())
```

	status: 200
	headers: <CIMultiDictProxy('Date': 'Tue, 22 Feb 2022 14:55:52 GMT', 'Content-Type': 'application/json', 'Content-Length': '519', 'Connection': 'keep-alive', 'Server': 'gunicorn/19.9.0', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true')>
	body: {
	  "args": {}, 
	  "data": "{\"name\": \"iheng\", \"age\": 23}", 
	  "files": {}, 
	  "form": {}, 
	  "headers": {
		"Accept": "*/*", 
		"Accept-Encoding": "gzip, deflate", 
		"Content-Length": "28", 
		"Content-Type": "application/json", 
		"Host": "www.httpbin.org", 
		"User-Agent": "Python/3.9 aiohttp/3.8.1", 
		"X-Amzn-Trace-Id": "Root=1-6214f978-12f493b25646ef991225b24f"
	  }, 
	  "json": {
		"age": 23, 
		"name": "iheng"
	  }, 
	  "origin": "36.28.12.6", 
	  "url": "https://www.httpbin.org/post"
	}

	bytes: b'{\n  "args": {}, \n  "data": "{\\"name\\": \\"iheng\\", \\"age\\": 23}", \n  "files": {}, \n  "form": {}, \n  "headers": {\n    "Accept": "*/*", \n    "Accept-Encoding": "gzip, deflate", \n    "Content-Length": "28", \n    "Content-Type": "application/json", \n    "Host": "www.httpbin.org", \n    "User-Agent": "Python/3.9 aiohttp/3.8.1", \n    "X-Amzn-Trace-Id": "Root=1-6214f978-12f493b25646ef991225b24f"\n  }, \n  "json": {\n    "age": 23, \n    "name": "iheng"\n  }, \n  "origin": "36.28.12.6", \n  "url": "https://www.httpbin.org/post"\n}\n'
	bytes: {'args': {}, 'data': '{"name": "iheng", "age": 23}', 'files': {}, 'form': {}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Length': '28', 'Content-Type': 'application/json', 'Host': 'www.httpbin.org', 'User-Agent': 'Python/3.9 aiohttp/3.8.1', 'X-Amzn-Trace-Id': 'Root=1-6214f978-12f493b25646ef991225b24f'}, 'json': {'age': 23, 'name': 'iheng'}, 'origin': '36.28.12.6', 'url': 'https://www.httpbin.org/post'}
	
### 超时设置
```
import aiohttp
import asyncio

async def main():
    timeout = aiohttp.ClientTimeout(total=1)
    async with aiohttp.ClientSession(timeout=timeout) as session:
        async with session.get('https://www.httpbin.org/get') as response:
            print('status:',response.status)

if __name__ == "__main__":
    asyncio.get_event_loop().run_until_complete(main())
```

	raise asyncio.TimeoutError from None
	asyncio.exceptions.TimeoutError
	
### 并发限制
```
import aiohttp
import asyncio
import logging
logging.basicConfig(level=logging.INFO,format="%(levelname)s - %(asctime)s:%(message)s")

CONCURRENCY = 5
URL = 'https://www.baidu.com'
semaphore = asyncio.Semaphore(CONCURRENCY)
session = None

async def scrape_api():
    async with semaphore:
        logging.info('scraping %s',URL)
        async with session.get(URL) as response:
            await asyncio.sleep(1)

            return await response.text()

async def main():
    global session
    session = aiohttp.ClientSession()
    scrape_index_tasks = [asyncio.ensure_future(scrape_api()) for _ in range(10000)]
    await asyncio.gather(*scrape_index_tasks)

if __name__ == "__main__":
    asyncio.get_event_loop().run_until_complete(main())
```

## aiohttp异步爬取实战
### 案例介绍
### 准备工作
### 页面分析
```
import asyncio
import aiohttp
import logging

logging.basicConfig(level=logging.INFO,format="%(levelname)s - %(asctime)s : %(message)s")
INDEX_URL = "https://spa5.scrape.center/api/book/?limit=18&offset={offset}"
DETAIL_URL = "https://spa5.scrape.center/api/book/{id}/"
PAGE_SIZE = 18
PAGE_NUMBER = 10
CONCURRENCY = 5
```

```
async def scrape_api(url):
    async with semaphore:
        try:
            logging.info('scraping %s',url)
            async with session.get(url) as response:
                time.sleep(1)
                return await response.json()
        except aiohttp.ClientError:
            logging.error('error occurred while scraping %s',url,exc_info=True)
```

```
async def scrape_index(page):
    url = INDEX_URL.format(offset = PAGE_SIZE*(page-1))
    return await scrape_api(url)
```

```
import json

async def main():
    global session
    session = aiohttp.ClientSession()
    scrape_index_tasks = [asyncio.ensure_future(scrape_index(page))for page in range(1,PAGE_NUMBER+1)]
    results = await asyncio.gather(*scrape_index_tasks)
    logging.info('result %s',results)
	
if __name__ == "__main__":
	asyncio.get_event_loop().run_until_complete(main())
```

```
ids = []
for index_data in results:
	if not index_data:continue
	for item in index_data['results']:
		ids.append(item['id'])	
```

```
from motor.motor_asyncio import AsyncIOMotorClient

MONGO_CONNECTION_STRING = 'mongodb://127.0.0.1:27017'
MONGO_DB_NAME = "books"
MONGO_COLLECTION_NAME = 'books'

client = AsyncIOMotorClient(MONGO_CONNECTION_STRING)
db = client[MONGO_DB_NAME]
collection = db[MONGO_COLLECTION_NAME]

async def save_data(data):
    logging.info('saving data %s',data)
    if data:
        return await collection.update_one(
            {
                "id":data.get('id')
            },
            {
                "$set":data
            },upsert = True)
async def scrape_detail(id):
    url = DETAIL_URL.format(id=id)
    data = await scrape_api(url)
    await save_data(data)
```
```
scrape_detail_tasks = [asyncio.ensure_future(scrape_detail(id))for id in ids]
await asyncio.gather(*scrape_detail_tasks)
await session.close()
```