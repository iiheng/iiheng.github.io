---
title: 代理的使用
date: 2022-02-27 20:29:29
tags:
categories: python爬虫的学习
---
## 前言
- 爬虫文章均来自《python3网络爬虫开发实战教程》，只为方便记录日常所学，以及方便观看。

## 代理的设置
### 准备工作
### urllib的代理设置
```
from urllib.error import URLError
from urllib.request import ProxyHandler,build_opener

proxy = "127.0.0.1:7890"
proxy_handler = ProxyHandler({
    'http':'http://'+proxy,
    'https':'https://'+proxy
})

opener = build_opener(proxy_handler)
try:
    response = opener.open("https://www.httpbin.org/get")
    print(response.read().decode('utf-8'))
except URLError as e:
    print(e.reason)
```

	{
	  "args": {}, 
	  "headers": {
		"Accept-Encoding": "identity", 
		"Host": "www.httpbin.org", 
		"User-Agent": "Python-urllib/3.9", 
		"X-Amzn-Trace-Id": "Root=1-6221c2c4-3f12332e05515dce190ad5d9"
	  }, 
	  "origin": "172.104.141.246", 
	  "url": "https://www.httpbin.org/get"
	}

```
from urllib.error import URLError
from urllib.request import ProxyHandler,build_opener

proxy = "username:password@127.0.0.1:7890"
proxy_handler = ProxyHandler({
    'http':'http://'+proxy,
    'https':'http://'+proxy
})

opener = build_opener(proxy_handler)
try:
    response = opener.open("https://www.httpbin.org/get")
    print(response.read().decode('utf-8'))
except URLError as e:
    print(e.reason)
```
```
import socks
import socket
from urllib.error import URLError
from urllib import request

socks.set_default_proxy(socks.SOCKS5,'127.0.0.1',7891)
socket.socket = socks.socksocket
try:
    response = request.urlopen("https://www.httpbin.org/get")
    print(response.read().decode('utf-8'))
except URLError as e:
    print(e.reason)
```

	{
	  "args": {}, 
	  "headers": {
		"Accept-Encoding": "identity", 
		"Host": "www.httpbin.org", 
		"User-Agent": "Python-urllib/3.9", 
		"X-Amzn-Trace-Id": "Root=1-6221c7c8-37c838ab0bdd63f5650bfd6b"
	  }, 
	  "origin": "115.200.163.126", 
	  "url": "https://www.httpbin.org/get"
	}
	

### requests的代理设置
```
import requests

proxy = "127.0.0.1:7890"
proxies = {
    'http':'http://'+proxy,
    'https':'http://'+proxy
}
try:
    response = requests.get("https://www.httpbin.org/get",proxies=proxies)
    print(response.text)
except requests.exceptions.ConnectionError as e:
    print('Error:',e.args)
```

	{
	  "args": {}, 
	  "headers": {
		"Accept": "*/*", 
		"Accept-Encoding": "gzip, deflate", 
		"Host": "www.httpbin.org", 
		"User-Agent": "python-requests/2.27.1", 
		"X-Amzn-Trace-Id": "Root=1-6221cb63-703203524eb7eb98122f9b27"
	  }, 
	  "origin": "172.104.141.246", 
	  "url": "https://www.httpbin.org/get"
	}
	
`proxy = 'username:password@127.0.0.1:7890'`

```
import requests

proxy = "127.0.0.1:7891"
proxies = {
    'http':'socks5://'+proxy,
    'https':'socks5://'+proxy
}
try:
    response = requests.get("https://www.httpbin.org/get",proxies=proxies)
    print(response.text)
except requests.exceptions.ConnectionError as e:
    print('Error:',e.args)

```

```
import requests
import socks
import socket

socks.set_default_proxy(socks.SOCKS5,'127.0.0.1',7891)
socket.socket = socks.socksocket
try:
    response = requests.get("https://www.httpbin.org/get")
    print(response.text)
except requests.exceptions.ConnectionError as e:
    print('Error:',e.args)
```
### httpx的代理设置
```
import httpx

proxy = "127.0.0.1:7890"
proxies = {
    'http://':'http://'+proxy,
    "https://":"http://"+proxy
}

with httpx.Client(proxies=proxies) as client:
    response = client.get("https://www.httpbin.org/get")
    print(response.text)
```

	{
	  "args": {}, 
	  "headers": {
		"Accept": "*/*", 
		"Accept-Encoding": "gzip, deflate", 
		"Host": "www.httpbin.org", 
		"User-Agent": "python-httpx/0.22.0", 
		"X-Amzn-Trace-Id": "Root=1-6221cf16-31b390d1781899b66bb728ed"
	  }, 
	  "origin": "172.104.141.246", 
	  "url": "https://www.httpbin.org/get"
	}
	
`proxy = "username:password@127.0.0.1:7890"`

`pip install "httpx-socks[asyncio]"`

```
import httpx
from httpx_socks import SyncProxyTransport

transport = SyncProxyTransport.from_url('socks5://127.0.0.1:7891')

with httpx.Client(transport=transport) as client:
    response = client.get("https://www.httpbin.org/get")
    print(response.text)
```

```
import httpx
import asyncio
from httpx_socks import AsyncProxyTransport

transport = AsyncProxyTransport.from_url('socks5://127.0.0.1:7891')

async def main():
    async with httpx.AsyncClient(transport=transport) as client:
        response = await client.get('https://www.httpbin.org/get')
        print(response.text)

if __name__=="__main__":
    asyncio.get_event_loop().run_until_complete(main())
```
### Selenium的代理设置
```
from selenium import webdriver


proxy = '127.0.0.1:7890'
options = webdriver.ChromeOptions()
options.add_argument('--proxy-server=http://'+proxy)
browser = webdriver.Chrome(options=options)
browser.get('https://www.httpbin.org/get')
print(browser.page_source)
browser.close()
```
	<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">{
	  "args": {}, 
	  "headers": {
		"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9", 
		"Accept-Encoding": "gzip, deflate, br", 
		"Accept-Language": "zh-CN,zh;q=0.9", 
		"Host": "www.httpbin.org", 
		"Sec-Ch-Ua": "\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"98\", \"Google Chrome\";v=\"98\"", 
		"Sec-Ch-Ua-Mobile": "?0", 
		"Sec-Ch-Ua-Platform": "\"Windows\"", 
		"Sec-Fetch-Dest": "document", 
		"Sec-Fetch-Mode": "navigate", 
		"Sec-Fetch-Site": "none", 
		"Sec-Fetch-User": "?1", 
		"Upgrade-Insecure-Requests": "1", 
		"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36", 
		"X-Amzn-Trace-Id": "Root=1-6221d337-1f9c93ce0901d72527b539cd"
	  }, 
	  "origin": "172.104.141.246", 
	  "url": "https://www.httpbin.org/get"
	}
	</pre></body></html>
```
from selenium import webdriver

proxy = '127.0.0.1:7890'
options = webdriver.ChromeOptions()
options.add_argument('--proxy-server=socks5://'+proxy)
browser = webdriver.Chrome(options=options)
browser.get('https://www.httpbin.org/get')
print(browser.page_source)
browser.close()
```
### aiohttp的代理设置
```
import asyncio
import aiohttp

proxy = "http://127.0.0.1:7890"
async def main():
    async with aiohttp.ClientSession() as session:
        async with session.get('https://www.httpbin.org/get',proxy = proxy) as response:
            print(await response.text())

if __name__ == "__main__":
    asyncio.get_event_loop().run_until_complete(main())
```

	{
	  "args": {}, 
	  "headers": {
		"Accept": "*/*", 
		"Accept-Encoding": "gzip, deflate", 
		"Host": "www.httpbin.org", 
		"User-Agent": "Python/3.9 aiohttp/3.8.1", 
		"X-Amzn-Trace-Id": "Root=1-6221d46a-0152a3644c317554196dcadd"
	  }, 
	  "origin": "172.104.141.246", 
	  "url": "https://www.httpbin.org/get"
	}

`proxy = "http://username:password@127.0.0.1:7890"`

`pip3 install aiohttp-socks`

```
import asyncio
import aiohttp
from aiohttp_socks import ProxyConnector

connector = ProxyConnector.from_url('socks5://127.0.0.1:7890')
async def main():
    async with aiohttp.ClientSession(connector=connector) as session:
        async with session.get('https://www.httpbin.org/get') as response:
            print(await response.text())

if __name__ == "__main__":
    asyncio.get_event_loop().run_until_complete(main())
```

### Playwright的代理设置
```
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch(proxy={
        'server':'http://127.0.0.1:7890'
    })
    page = browser.new_page()
    page.goto("https://www.httpbin.org/get")
    print(page.content())
    browser.close()
```

	<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">{
	  "args": {}, 
	  "headers": {
		"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9", 
		"Accept-Encoding": "gzip, deflate, br", 
		"Host": "www.httpbin.org", 
		"Sec-Fetch-Dest": "document", 
		"Sec-Fetch-Mode": "navigate", 
		"Sec-Fetch-Site": "none", 
		"Sec-Fetch-User": "?1", 
		"Upgrade-Insecure-Requests": "1", 
		"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/100.0.4863.0 Safari/537.36", 
		"X-Amzn-Trace-Id": "Root=1-6221d86a-217af66272402cd62006ff7c"
	  }, 
	  "origin": "172.104.141.246", 
	  "url": "https://www.httpbin.org/get"
	}
	</pre></body></html>
	
```
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch(proxy={
        'server':'socks5://127.0.0.1:7890'
    })
    page = browser.new_page()
    page.goto("https://www.httpbin.org/get")
    print(page.content())
    browser.close()
```

```
from playwright.sync_api import sync_playwright

with sync_playwright() as p:
    browser = p.chromium.launch(proxy={
        'server':'socks5://127.0.0.1:7890',
        'username':'foo',
        'password':'bar'
    })
    page = browser.new_page()
    page.goto("https://www.httpbin.org/get")
    print(page.content())
    browser.close()
```

### 代理反爬案例爬取实战
#### 本节目标
#### 准备工作
`pip install requests redis environs pyquery loguru`

#### 爬取分析
> 构造Redis爬取队列，用队列存取请求
> 实现异常处理，把失败的请求重新加入队列
> 解析列表页的数据，将爬取详情页和下一页的请求加入队列
> 提取详情页的信息

#### 构造请求对象

	class Request(RequestHooksMixin):
		def __init__(self,
				method=None, url=None, headers=None, files=None, data=None,
				params=None, auth=None, cookies=None, hooks=None, json=None):

			# Default empty dicts for dict params.
			data = [] if data is None else data
			files = [] if files is None else files
			headers = {} if headers is None else headers
			params = {} if params is None else params
			hooks = {} if hooks is None else hooks

			self.hooks = default_hooks()
			for (k, v) in list(hooks.items()):
				self.register_hook(event=k, hook=v)

			self.method = method
			self.url = url
			self.headers = headers
			self.files = files
			self.data = data
			self.json = json
			self.params = params
			self.auth = auth
			self.cookies = cookies

    def __repr__(self):
        return '<Request [%s]>' % (self.method)


```
TIMEOUT = 10
from requests import Request

class MovieRequest(Request):
    def __init__(self,url,callback,method="Get",headers=None,need_proxy=False,fail_time=0,timeout=TIMEOUT):
        Request.__init__(self,method,url,headers)
        self.callback = callback
        self.fail_time =fail_time
        self.timeout = timeout
```

#### 实现请求队列
```
from pickle import dumps,loads
from redis import StrictRedis
from request import MovieRequest

class RedisQueue():
    def __init__(self):
        self.db = StrictRedis(host=REDIS_HOST,port=REDIS_PORT,password=REDIS_PASSWORD)

    def add(self,request):
        if isinstance(request,MovieRequest):
            return self.db.rpush(REDIS_KEY,dumps(request))
        return False

    def pop(self):
        if self.db.llen(REDIS_KEY):
            return loads(self.db.lpop(REDIS_KEY))
        return False

    def empty(self):
        return self.db.llen(REDIS_KEY) == 0
```

#### 修改代理池
```
import requests
PROXY_POOL_URL = "http://127.0.0.1:5555/random"
from loguru import logger

@logger.catch
def get_proxy():
    response = requests.get(PROXY_POOL_URL)
    if response.status_code == 200:
        logger.debug(f'get proxy {response.text}')
        return response.text
```

#### 第一个请求
```
from requests import Session

from db import RedisQueue
from request import MovieRequest
BASE_URL = "https://antispider5.scrape.center/"
HEADERS = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36 Edg/98.0.1108.62'
}

class Spider():
    session = Session()
    queue = RedisQueue()

    def start(self):
        self.session.headers.update(HEADERS)
        start_url = BASE_URL
        request = MovieRequest(url = start_url,callback=self.parse_index)
        self.queue.add(request)
```

#### 调度请求
